---
title: "Lista de Ejercicios de MPI"
description: "Conjunto completo de ejercicios prácticos de MPI que incluyen comunicación con tags, paralelización de algoritmos, comunicación no bloqueante, método de mínimos cuadrados, operaciones colectivas y análisis de rendimiento."
pubDate: "2025-09-17"
author: "Railly Hugo"
authorImageUrl: "/hugo-profile.webp"
dueDate: "2025-10-15"
files: ["ejercicios_mpi.zip"]
path: "Laboratory/MPI/exercises"
---

import Alert from "../../../components/Alert.astro";
import CodeBlock from "../../../components/CodeBlock.astro";
import Card from "../../../components/Card.astro";

<Alert type="info">
  **Importante:** Este conjunto de ejercicios debe realizarse después de completar la semana 4 del curso. Asegúrate de tener configurado correctamente tu entorno MPI antes de comenzar.
</Alert>

## Descripción General

Esta lista de ejercicios está diseñada para profundizar en conceptos avanzados de MPI, incluyendo técnicas de comunicación, paralelización de algoritmos secuenciales, manejo de deadlocks y análisis de rendimiento. Cada ejercicio debe incluir un **informe detallado** que explique la solución implementada.

**Repositorio de código:** [https://github.com/FISI-SM/parallelProgramming-repo/tree/main/exercises/MPI](https://github.com/FISI-SM/parallelProgramming-repo/tree/main/exercises/MPI)

---

## Ejercicio 1: Combinación de Mensajes utilizando Tags

### Archivos
- **Archivo de Consulta:** [`comunicacion_basica.c`](https://github.com/FISI-SM/parallelProgramming-repo/blob/main/exercises/MPI/comunicacion_basica.c)
- **Archivo de Respuesta:** `comunicacion_basica_solucion.c`

### Objetivo
Utilizar el parámetro `tag` en las funciones `MPI_Send` y `MPI_Recv` para distinguir diferentes tipos de mensajes en una aplicación MPI.

### Descripción del Problema
Una aplicación puede utilizar el parámetro tag en las funciones send y receive para distinguir mensajes. Utilice el programa `comunicacion_basica.c` y modifícalo para que:

1. El **maestro** envíe **dos mensajes** a cada esclavo, utilizando **diferentes tags**
2. Cada **esclavo** reciba los mensajes en **orden inverso**, utilizando las tags
3. Luego responda al maestro como en el archivo original

### Tareas a Realizar

1. **Análisis del código original**: Examina el comportamiento del programa base
2. **Implementación de tags**: Modifica el código para usar diferentes tags (por ejemplo, `TAG_MENSAJE1 = 1` y `TAG_MENSAJE2 = 2`)
3. **Recepción en orden inverso**: Los esclavos deben recibir primero el mensaje con tag 2, luego el mensaje con tag 1
4. **Documentación**: Explica en tu informe cómo los tags permiten la comunicación selectiva

**Ejemplo de uso de tags:**

<CodeBlock language="c" content={`#define TAG_DATO_A 100
#define TAG_DATO_B 200

// Enviar con tags diferentes
MPI_Send(&dato_a, 1, MPI_INT, destino, TAG_DATO_A, MPI_COMM_WORLD);
MPI_Send(&dato_b, 1, MPI_INT, destino, TAG_DATO_B, MPI_COMM_WORLD);

// Recibir en orden específico usando tags
MPI_Recv(&buffer_b, 1, MPI_INT, origen, TAG_DATO_B, MPI_COMM_WORLD, &status);
MPI_Recv(&buffer_a, 1, MPI_INT, origen, TAG_DATO_A, MPI_COMM_WORLD, &status);`} />

**Conceptos clave:**
- Los tags permiten identificar y ordenar mensajes específicos
- `MPI_Recv` puede especificar qué tag recibir
- Útil para protocolos de comunicación complejos

---

## Ejercicio 2: Convertir un código serial en paralelo

### Archivos
- **Archivo de Consulta:** [`aproximacion_pi.c`](https://github.com/FISI-SM/parallelProgramming-repo/blob/main/exercises/MPI/aproximacion_pi.c)
- **Archivo de Respuesta:** `aproximacion_pi_solucion.c`
- **Datos de Entrada:** `datos_valores`

### Objetivo
Paralelizar un algoritmo secuencial que calcula el valor de π usando una aproximación integral, implementando el enfoque SPMD (Single Program, Multiple Data).

### Descripción del Problema
El programa `aproximacion_pi.c` calcula el valor de π usando una aproximación integral. Debes modificar este algoritmo para crear una versión paralela utilizando el enfoque SPMD.

### Conceptos Clave
- **Aproximación integral**: π se calcula como ∫₀¹ 4/(1+x²) dx
- **SPMD**: Todos los procesos ejecutan el mismo programa pero trabajan con diferentes datos
- **Reducción**: Combinar resultados parciales de todos los procesos

### Tareas a Realizar

1. **Análisis del algoritmo secuencial**: Comprende cómo se calcula π en el código original
2. **Descomposición del dominio**: Divide el intervalo [0,1] entre los procesos disponibles
3. **Cálculo paralelo**: Cada proceso calcula su suma parcial
4. **Reducción**: Combina todas las sumas parciales para obtener el resultado final
5. **Análisis de rendimiento**: Compara tiempos de ejecución entre versión secuencial y paralela

**Patrón general de paralelización:**

<CodeBlock language="c" content={`// 1. Cada proceso determina su trabajo
int mi_inicio = /* calcular según rank */;
int mi_fin = /* calcular según rank */;

// 2. Procesamiento local
double suma_local = 0.0;
for (int i = mi_inicio; i < mi_fin; i++) {
    suma_local += /* función a integrar */;
}

// 3. Combinación de resultados
double suma_global;
MPI_Reduce(&suma_local, &suma_global, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);`} />

**Estrategia de solución:**
- **Descomposición del dominio**: Dividir el intervalo de integración entre procesos
- **Cálculo distribuido**: Cada proceso trabaja con su subconjunto de datos
- **Agregación de resultados**: Usar operaciones colectivas para combinar resultados parciales

---

## Ejercicio 3: Comunicación no bloqueante vs bloqueante

### Archivos
- **Archivo de Consulta:** [`bloqueo_mutuo.c`](https://github.com/FISI-SM/parallelProgramming-repo/blob/main/exercises/MPI/bloqueo_mutuo.c)
- **Archivo de Respuesta:** `bloqueo_mutuo_corregido.c`

### Objetivo
Identificar y corregir un problema de deadlock en comunicación MPI, reemplazando comunicación bloqueante por no bloqueante.

### Descripción del Problema
El programa `bloqueo_mutuo.c` presenta un deadlock cuando se ejecuta con dos nodos. El programa muestra algunas líneas y luego se cuelga, requiriendo terminar el proceso con Control+C.

### Análisis del Deadlock
Un deadlock ocurre cuando:
1. Dos procesos intentan enviarse datos mutuamente
2. Ambos usan `MPI_Send` (bloqueante) al mismo tiempo
3. Cada proceso espera que el otro reciba antes de continuar
4. El sistema se bloquea indefinidamente

### Tareas a Realizar

1. **Identificación del problema**: Analiza el código y explica por qué ocurre el deadlock
2. **Solución con comunicación no bloqueante**: Implementa `MPI_Isend` y `MPI_Irecv`
3. **Sincronización**: Usa `MPI_Wait` o `MPI_Waitall` para completar las operaciones
4. **Verificación**: Demuestra que el programa corregido termina correctamente

**Diferencia entre comunicación bloqueante y no bloqueante:**

<CodeBlock language="c" content={`// Comunicación bloqueante (puede causar deadlock)
MPI_Send(datos, tamaño, MPI_INT, destino, tag, MPI_COMM_WORLD);
MPI_Recv(buffer, tamaño, MPI_INT, origen, tag, MPI_COMM_WORLD, &status);

// Comunicación no bloqueante (evita deadlock)
MPI_Request request_send, request_recv;
MPI_Isend(datos, tamaño, MPI_INT, destino, tag, MPI_COMM_WORLD, &request_send);
MPI_Irecv(buffer, tamaño, MPI_INT, origen, tag, MPI_COMM_WORLD, &request_recv);

// Esperar a que terminen las operaciones
MPI_Wait(&request_send, MPI_STATUS_IGNORE);
MPI_Wait(&request_recv, &status);`} />

**Conceptos clave:**
- **Deadlock**: Cuando procesos esperan indefinidamente unos por otros
- **Funciones no bloqueantes**: Permiten continuar la ejecución sin esperar
- **MPI_Request**: Maneja el estado de operaciones asíncronas

---

## Ejercicio 4: Método de los mínimos cuadrados

### Archivos de Consulta
- [`minimos_cuadrados.c`](https://github.com/FISI-SM/parallelProgramming-repo/blob/main/exercises/MPI/minimos_cuadrados.c)
- [`pausa_personalizada.c`](https://github.com/FISI-SM/parallelProgramming-repo/blob/main/exercises/MPI/pausa_personalizada.c)
- [`minimos_cuadrados_serial.c`](https://github.com/FISI-SM/parallelProgramming-repo/blob/main/exercises/MPI/minimos_cuadrados_serial.c)
- [`datos_xy.txt`](https://github.com/FISI-SM/parallelProgramming-repo/blob/main/exercises/MPI/datos_xy.txt)

### Objetivo
Implementar y optimizar el método de mínimos cuadrados en paralelo, explorando diferentes estrategias de comunicación y balanceo de carga.

### Descripción del Problema
El método de los mínimos cuadrados es una técnica de optimización matemática para encontrar el mejor ajuste para un conjunto de datos, minimizando la suma de los cuadrados de las diferencias entre valores estimados y observados.

### Sub-ejercicios

#### 4.1 Análisis del código base
- Estudia el algoritmo y comprende la descomposición de datos
- Analiza las comunicaciones bloqueantes usadas
- Compara con la versión serial

#### 4.2 Compilación y ejecución
**Ejemplo de compilación:**

<CodeBlock language="bash" content={`# Si usas pausa_personalizada (para ciertos procesadores)
gcc -c pausa_personalizada.c -o pausa_personalizada.o
mpicc minimos_cuadrados.c pausa_personalizada.o -o programa -lm

# Compilación estándar
mpicc minimos_cuadrados.c -o programa -lm`} />

#### 4.3 Experimentación con procesos
- Ejecuta con diferentes números de procesos (2-10)
- Analiza el comportamiento y rendimiento

#### 4.4 Comunicación no bloqueante
- Reemplaza `MPI_Send`/`MPI_Recv` por `MPI_Isend`/`MPI_Irecv`
- Compara rendimiento entre ambas versiones

#### 4.5 Balanceo de carga
El programa original no está optimizado ya que el último proceso maneja más datos. Implementa un balanceo de carga más equitativo.

#### 4.6 Reducción con árbol binario
Reescribe el paso 4 (donde el proceso 0 recibe todas las sumas parciales) usando un árbol binario:

1. Divide los procesos en dos grupos
2. Cada proceso del grupo 2 envía su suma parcial al grupo 1
3. Repite la división hasta que el proceso 0 tenga todas las sumas

**Implementación del árbol binario:**
- **Fase de reducción**: Los procesos se organizan en niveles jerárquicos
- **Patrón de comunicación**: En cada nivel, la mitad de los procesos envían datos
- **Eficiencia**: Reducir la complejidad de O(n) a O(log n)
- **Desafío**: Manejar números impares de procesos correctamente

---

## Ejercicio 5: Práctica de Comunicación Colectiva

### Archivos
- **Archivo de Consulta:** [`comunicacion_colectiva.c`](https://github.com/FISI-SM/parallelProgramming-repo/blob/main/exercises/MPI/comunicacion_colectiva.c)
- **Archivo de Datos:** [`semilla.seed`](https://github.com/FISI-SM/parallelProgramming-repo/blob/main/exercises/MPI/semilla.seed)
- **Archivos de Respuesta:** `comunicacion_colectiva_solucion.c`, `datos_salida.data`

### Objetivo
Implementar operaciones de comunicación colectiva en MPI para distribución de datos y cálculo de estadísticas distribuidas.

### Estructura del Programa

1. **Proceso 0** lee un número aleatorio (`semilla.seed`)
2. Ese número se **envía a todos los otros procesos**
3. Cada proceso **calcula un número aleatorio** basado en el número recibido y su rank
4. El proceso con **mayor rank** calcula el **valor medio** de los números aleatorios
5. **Cuatro nuevos números aleatorios** son generados por cada nodo
6. Se calculan el **valor máximo** y la **desviación estándar** de todos los números

### Tareas a Realizar

#### 5.1 Análisis estructural
- Identifica cada uno de los 6 pasos en el código
- Comprende la función `ObtenerEstadisticas` que calcula máximo y desviación estándar

#### 5.2 Implementación de comunicaciones colectivas
Inserta las rutinas MPI apropiadas en los puntos indicados:

**Ejemplos de operaciones colectivas:**

<CodeBlock language="c" content={`// Distribuir datos desde el proceso 0
int semilla;
MPI_Bcast(&semilla, 1, MPI_INT, 0, MPI_COMM_WORLD);

// Recolectar datos en un proceso específico
double mi_valor, todos_los_valores[comm_size];
MPI_Gather(&mi_valor, 1, MPI_DOUBLE, todos_los_valores, 1, MPI_DOUBLE,
           proceso_destino, MPI_COMM_WORLD);

// Operación global (todos obtienen el resultado)
double valor_local, valor_maximo;
MPI_Allreduce(&valor_local, &valor_maximo, 1, MPI_DOUBLE, MPI_MAX, MPI_COMM_WORLD);`} />

**Operaciones colectivas a implementar:**
- **MPI_Bcast**: Para distribuir la semilla inicial
- **MPI_Gather**: Para recolectar datos en un proceso específico
- **MPI_Allreduce**: Para operaciones globales (máximo, suma, etc.)

#### 5.3 Compilación y pruebas

<CodeBlock language="bash" content={`mpicc -o comunicacion_colectiva comunicacion_colectiva.c -lm`} />

#### 5.4 Verificación
- Ejecuta con 1-8 procesos
- Para verificar exactitud, cambia `semilla.seed` a `123456`
- Compara tu salida con `datos_salida.data`

---

## Ejercicio 6: Práctica de Comunicación Punto a Punto

### Archivos
- **Archivo de Consulta:** [`latencia_mpi.c`](https://github.com/FISI-SM/parallelProgramming-repo/blob/main/exercises/MPI/latencia_mpi.c)
- **Archivo de Consulta:** [`ancho_banda_mpi.c`](https://github.com/FISI-SM/parallelProgramming-repo/blob/main/exercises/MPI/ancho_banda_mpi.c)

### Objetivo
Medir y analizar el rendimiento de comunicaciones punto a punto en MPI, determinando latencia y ancho de banda de la red.

### Parte A: Medición de Latencia

#### Conceptos
- **Latencia**: Tiempo mínimo para enviar un mensaje (generalmente medido con mensajes pequeños)
- **Ping-pong**: Método donde dos procesos se envían mensajes repetidamente

#### Tareas
1. **Análisis del código**: Comprende cómo funciona `latencia_mpi.c`
2. **Compilación**: Compila el programa normalmente
3. **Ejecución**: Ejecuta con exactamente 2 nodos y 2 procesos
4. **Medición**: Registra los tiempos de latencia obtenidos

### Parte B: Medición de Ancho de Banda

#### Conceptos
- **Ancho de banda**: Cantidad máxima de datos que se pueden transmitir por unidad de tiempo
- **Throughput**: Rendimiento real de transferencia de datos

#### Tareas
1. **Análisis del código**: Estudia `ancho_banda_mpi.c` y comprende los parámetros
2. **Experimento 1**: Varía el número de procesos y explica cómo cambia el ancho de banda
3. **Experimento 2**: Fija 2 procesos y modifica el tamaño del mensaje:
   - 10,000 elementos
   - 100,000 elementos
   - 1,000,000 elementos
4. **Análisis**: ¿Qué patrones observas en el ancho de banda con diferentes tamaños?

<Alert type="warning">
  **Nota:** Para mensajes muy largos, disminuye el número de repeticiones para evitar tiempos de ejecución excesivos.
</Alert>

**Métricas a calcular:**
- **Latencia**: Tiempo mínimo de transmisión (mensajes pequeños)
- **Ancho de banda**: Throughput máximo (mensajes grandes)
- **Variación con el tamaño**: Cómo cambia el rendimiento según el tamaño del mensaje
- **Efecto del número de procesos**: Impacto en el rendimiento de la red

---

## Entregables

### Formato del Informe
Cada ejercicio debe incluir:

1. **Análisis del problema**: Descripción del problema y approach de solución
2. **Implementación**: Código fuente con comentarios explicativos
3. **Resultados**: Salidas del programa y mediciones de rendimiento
4. **Análisis comparativo**: Comparación entre versiones secuencial/paralela cuando aplique
5. **Conclusiones**: Lecciones aprendidas y observaciones importantes

### Estructura de Archivos
```
ejercicios_mpi/
├── ejercicio1/
│   ├── comunicacion_basica_solucion.c
│   └── informe_ejercicio1.md
├── ejercicio2/
│   ├── aproximacion_pi_solucion.c
│   └── informe_ejercicio2.md
├── ejercicio3/
│   ├── bloqueo_mutuo_corregido.c
│   └── informe_ejercicio3.md
├── ejercicio4/
│   ├── minimos_cuadrados_optimizado.c
│   └── informe_ejercicio4.md
├── ejercicio5/
│   ├── comunicacion_colectiva_solucion.c
│   └── informe_ejercicio5.md
├── ejercicio6/
│   ├── analisis_rendimiento.txt
│   └── informe_ejercicio6.md
└── README.md
```

### Criterios de Evaluación
- **Correctitud del código (40%)**: Los programas compilan y ejecutan correctamente
- **Calidad del informe (30%)**: Análisis detallado y explicaciones claras
- **Optimización (20%)**: Implementación eficiente y uso apropiado de MPI
- **Experimentación (10%)**: Pruebas con diferentes configuraciones y análisis de resultados

<Alert type="success">
  **Tip:** Utiliza herramientas como `time` o `mpirun --hostfile` para medir rendimiento y probar en diferentes configuraciones de nodos.
</Alert>